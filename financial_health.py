# -*- coding: utf-8 -*-
"""Financial_health.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RCkNoYR1suYWb6p_AQs163Q4_60rRB0i
"""

# 📁 backend.py
import os
import pandas as pd
import requests
from pinecone import Pinecone, ServerlessSpec
from langgraph.graph import StateGraph
from typing import TypedDict

# --- LLM Config ---
API_URL = os.getenv("LLM_API_URL", "https://gptlab.rd.tuni.fi/students/ollama/v1/completions")
EMBEDDING_URL = "https://gptlab.rd.tuni.fi/students/ollama/v1/embeddings"
API_KEY = os.getenv("LLM_API_KEY", "sk-ollama-gptlab-sami-student-c2dd9657919cf45b6ff3f4b328416d89")
MODEL_NAME = os.getenv("LLM_MODEL", "llama3.1:latest")
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

def ask_llm(prompt: str):
    body = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "stream": False
    }
    response = requests.post(API_URL, headers=HEADERS, json=body)
    response.raise_for_status()

    json_data = response.json()
    return json_data["choices"][0]["text"]



def embed_question(text):
    try:
        res = requests.post(
            EMBEDDING_URL,
            headers=HEADERS,
            json={"model": MODEL_NAME, "input": text}
        )
        res.raise_for_status()
        embedding = res.json()["data"][0]["embedding"]
        # Truncate or pad the embedding to 384 dimensions
        embedding = embedding[:384] if len(embedding) >= 384 else embedding + [0.0] * (384 - len(embedding))
        return embedding
    except Exception as e:
        print("Embedding failed:", e)
        return [0.0] * 384  # fallback


# === CLEAN, CALCULATE, AND STORE ===
def clean_calculate_store(filepath="Financials.csv"):
    df = pd.read_csv(filepath)
    df.columns = df.columns.str.strip()

    money_cols = [
        "Units Sold", "Manufacturing Price", "Sale Price", "Gross Sales",
        "COGS", "Profit", "Sales"
    ]
    for col in money_cols:
        df[col] = (
            df[col]
            .astype(str)
            .str.replace("$", "", regex=False)
            .str.replace(",", "")
            .str.strip()
            .replace(["-", "–", "—", " - ", " -", "- ", " -   "], "0")
            .replace("", "0")
            .str.replace(r"[()]", "", regex=True)
            .apply(lambda x: f"-{x}" if "(" in x else x)
            .astype(float)
        )

    df["Date"] = pd.to_datetime(df["Date"], errors="coerce")
    df = df.dropna(subset=["Date"])

    total_revenue = df["Sales"].sum()
    total_expense = df["COGS"].sum()
    burn_rate = df["COGS"].mean()
    runway = (total_revenue - total_expense) / burn_rate if burn_rate else 0

    metrics = {
        "total_revenue": total_revenue,
        "total_expense": total_expense,
        "burn_rate": burn_rate,
        "runway": runway
    }

    chunks = df.apply(lambda row: " | ".join(f"{col}: {val}" for col, val in row.items()), axis=1).tolist()

    pc = Pinecone(api_key="pcsk_nLrJq_57LQZnfdxTCZ85PvVhwnEQ7qg3B4Uyg9YNemq1kQpNbwkEBRYxnw1bTHryhrnLp")

    index = pc.Index("financial-health")

    for i, text in enumerate(chunks):
        embedding = embed_question(text)
        index.upsert([(f"entry-{i}", embedding, {"text": text})])

    return metrics, df

# === LANGGRAPH AGENT ===
def retrieve_from_pinecone_tool(state):
    """Retrieve relevant context from Pinecone based on the user's question."""
    query = state["question"]
    q_vec = embed_question(query)
    pc = Pinecone(api_key="pcsk_nLrJq_57LQZnfdxTCZ85PvVhwnEQ7qg3B4Uyg9YNemq1kQpNbwkEBRYxnw1bTHryhrnLp")
    index = pc.Index("financial-health")
    search = index.query(vector=q_vec, top_k=5, include_metadata=True)
    context = "\n\n".join([match["metadata"]["text"] for match in search["matches"]])
    return {"context": context, "question": query}

def generate_answer_tool(state):
    """Generate an answer using the LLM and the retrieved context."""
    question = state["question"]
    context = state["context"]
    prompt = f"""You are a helpful financial assistant.\nUse the context below to answer the user's question.\n\nContext:\n{context}\n\nQuestion:\n{question}\n"""
    response = ask_llm(prompt)
    return {"response": response}


from langgraph.graph import StateGraph

def ask_agent(question: str):
    # Define a graph with state as a dict
    graph = StateGraph(dict)
    graph.add_node("Retriever", retrieve_from_pinecone_tool)
    graph.add_node("LLM", generate_answer_tool)

    # Set graph flow
    graph.set_entry_point("Retriever")
    graph.add_edge("Retriever", "LLM")
    graph.set_finish_point("LLM")

    # Compile and run
    agent = graph.compile()
    result = agent.invoke({"question": question})
    return result["response"]





